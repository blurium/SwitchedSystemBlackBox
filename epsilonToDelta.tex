% !TEX root = writeUpMainTheorems.tex
We denote $\epsilon':=\frac{\epsilon}{2} \sqrt{\frac{\lambda_{\max}(P)^n}{\det(P)}}$, where the additional factor $\frac{1}{2}$ follows from the homogeneity of the dynamics. In this section, we show how to relate $\epsilon'$ to $\delta$ in the statement of the Theorem \ref{thm:mainTheorem}. We start by a few definitions that will help us along the way.

\begin{definition}[Supporting Hyperplane] The hyperplane $H=\{x | c^Tx = k\}$ is a supporting hyperplane of the set $X \subseteq \R^n$ if $X$ is contained either in the set $\{x | c^Tx \leq k\}$ or $\{x | c^Tx \geq k\}$ and $X$ contains one point $z \in X$. Then $H$ is the supporting hyperplane of $X$ at $z$. We denote the set of all supporting hyperplanes of a set $X$ at point $z$ by $H_z(X)$.
\end{definition}

\begin{definition}[Spherical Cap]We define the \emph{spherical cap} on $S$ for a given hyperplane $c^Tx = k$ as:
\begin{equation*}\calC_{c,k} := \{x \in \sphere : c^Tx >k\}.\end{equation*}
\end{definition}

%\begin{proposition}Let $X \subset \R^n$ be a closed convex set. The set $X$ can be written as the intersection of all its support vectors:
%\begin{equation*} X = \cap_{i \in \partial X} c_i^Tx \leq k_i\end{equation*}
%\end{proposition}


\begin{proposition}\label{distanceSet} Let $\dist$ be a distance on $\R^n$. The distance between a set $X \subset \R^n$ and a point $p \in \R^n$ is $\dist(X, p):=\inf_{x \in X}\dist(x,p)$. Then if $X$ is compact, we have:
\begin{equation*}\dist(X,p)= \min_{x \in X}\dist(x,p). \end{equation*} 
\end{proposition}

\begin{proof}This is due to the fact that the function $\dist$ is continuous and therefore attains its minimum on the compact set $X$.
\end{proof}

\begin{proposition}[see e.g. \cite{boyd}]\label{prop:distance}The distance between the point $x=0$ and the hyperplane $c^Tx = k$ is $\frac{|k|}{\|c\|}$.
\end{proposition}

%\begin{lemma}\label{prop:distanceSet}Consider the boundary of the compact convex set
%\begin{equation*}X := \bigcap\limits_{i\in I}\{x: c_i^Tx \leq k_i\},\end{equation*} denoted by $\partial X$, then:
%\begin{equation*} \dist(\partial X,0) = \min_{i \in I} \frac{|k_i|}{\|c_i\|}.\end{equation*}
%\end{lemma}

%\begin{proof}Due to Proposition \ref{prop:distance} we have:
%\begin{equation}\dist(\partial X,0)= \inf_{i \in I} \dist\left(\{x: c_i^Tx = k_i\},0\right) = \inf_{i \in I}\koverc.
%\end{equation}
%Note that due to Proposition \ref{distanceSet}, the $\inf_{x \in \partial X}\dist(x,0)$ is attained by a point $x^* \in\partial X$ due to compactness of $\partial X$. Hence, there exists an $i \in I$ such that $c_i^Tx^*=k_i$ and this leads to: $\inf_{i \in I}\koverc = \min_{i \in I}\koverc$.
%\end{proof}

We define the function $\Delta : 2^\sphere \to [0,1]$ as:
\begin{equation}\label{shrinkage}
\Delta(X) = \sup \{r: \ball_r \subseteq \conv(\sphere \setminus X)\}.
\end{equation}

\begin{lemma}\label{lemma:delta2}$\Delta(\calC_{c,k}) = \min\left(1, \frac{|k|}{\|c\|}\right)$.
\end{lemma}

\begin{proof}
Note that $\conv(\sphere \setminus \calC_{c, k}) = \left\{x \in \ball : c^Tx \leq k \right\}$. Let \mbox{$X_{\calC_{c,k}} := \conv(\sphere \setminus \calC_{c, k})$.}
\begin{eqnarray}
\nonumber \Delta(X) &=& \dist(\partial X_{\calC_{c,k}}, 0) \\
\nonumber &=& \min(\dist(\sphere, 0), \dist(\{x : c^Tx = k\}, 0)) \\
\nonumber &=& \min\left(1, \frac{|k|}{\|c\|}\right).
\end{eqnarray}
\end{proof}
%\begin{lemma}\label{lemma:delta2}Given a set $X \subseteq \sphere$, let $\conv(\sphere \setminus X) := \ball \cap \calP$. Then we have:
%$$\Delta(X) = \min\left(\min_{i \in \partial X} \frac{|k_i|}{\|c_i\|}, 1\right),$$
%where $\left\{x \middle | c_i^Tx = k_i\right\} \in H_x(\calP)$ and $H \subseteq \{x : c_i^Tx \leq k_i\}$.
%\end{lemma}
%
%\begin{proof}
%Note that, $\calP = \cap_{i \in \partial X}\left\{x \middle | c_i^Tx \leq k_i\right\}$.
%\begin{eqnarray}
%\nonumber \Delta(X) &=& \sup \{r: \ball_r \subseteq \conv(\sphere \setminus X)\} \\
%\nonumber&=&\min \left(\dist(\partial \calP, 0), \dist(\sphere, 0) \right) \\
%\label{shrinkage1}&=&\min\left(\min_{i \in \partial X} \frac{|k_i|}{\|c_i\|},1\right) \qquad \qquad\text{due to Lemma\,} 
%{\ref{prop:distanceSet}}.
%\end{eqnarray}
%\end{proof}

\begin{corollary}\label{lemma:deltaMonotone}$\Delta(\calC_{c,k_1}) < \Delta(\calC_{c,k_2})$ when $k_1 < k_2$.
\end{corollary}

\begin{lemma}\label{lemma:muMonotone}$\sigma^{n-1}(\calC_{c,k_1}) < \sigma^{n-1}(\calC_{c,k_2})$, for $k_1 > k_2$.
\end{lemma}

\begin{proof}$\conv(\sphere \setminus \{x\in \sphere: c^Tx >k_1\}) \subseteq \conv(\sphere \setminus \{x\in \sphere: c^Tx >k_2\})$, for $k_1 > k_2$.
\end{proof}

Now we are ready to present the following lemma which is the key to proving our main result.
\begin{lemma}\label{lemma:constructSC}For any set $X \subseteq \sphere$, there exists $c$ and $k$ such that $\calC_{c,k}$ satisfies:
\begin{equation*}\calC_{c,k} \subseteq X,
\end{equation*}
 and
\begin{equation}\Delta(\calC_{c,k}) = \Delta(X).
\end{equation}
\end{lemma}

\begin{proof} Let $X_S := \conv(S \setminus X)$. Note that when $X_S= \emptyset$, the statement of the theorem holds trivially. So for the rest of the proof we assume that $X_S \not = \emptyset$. 
Since the distance function $\dist$ continuous and the set $\partial X_S$ is compact there exists a point $x^* \in \partial X_S$, such that:
\begin{equation}\label{deltaSupporting}\Delta(X) = \dist(\partial X_S, 0) = \inf_{x \in \partial X_S}\dist(x, 0) = \min_{x \in \partial X_S}\dist(x, 0) = \dist(x^*, 0).\end{equation} Then, by the supporting hyperplane theorem \cite{boyd}, there exists a supporting hyperplane $\left\{x : c^Tx = k\right\}$ of $\partial X_S$ such that $x^* \in \left\{x : c^Tx = k\right\}$. Note that:
\begin{equation*}\Delta(X) =  \dist(x^*, 0) = \dist(\{x : c^Tx = k\}) = \frac{|k|}{\|c\|}.
\end{equation*}
Now, consider the spherical cap $\calC_{c,k}$. Then, by Lemma \label{lemma:delta2} we have
$\Delta(\calC_{c,k}) = \frac{|k|}{\|c\|}$. Therefore, $\Delta(X) = \Delta(\calC_{c,k})$.

We next show $\calC_{c_\ell,k_\ell} \subseteq X$. We prove this by contradiction. Assume $x \in \calC_{c_\ell,k_\ell}$ and $x \notin X$. Note that, if $x \notin X$, then $x \in \sphere \setminus X \subseteq \conv(\sphere \setminus X).$ Since $x \in \calC_{c_\ell,k_\ell}$ we have $c_\ell^Tx>k_\ell$, but due to the fact that $x \in \conv(\sphere \setminus X)$, we also have $c_\ell^Tx \leq k_\ell$, which leads to a contradiction. Therefore, $\calC_{c_\ell,k_\ell} \subseteq X$. 
\end{proof}
%\begin{lemma} The function $\tilde{\lambda}_{max} : \R^{n(n-1)/2} \to \R$ is convex in $z$.
%\end{lemma}

%\begin{proof}Let the function $\phi: \mathbb{S}^{n} \to \R^{n(n-1)/2}$ map a given symmetric matrix to a vector of its elements. Then we have $\tilde{\lambda}_{max}(z) = \lambda_{max}(\phi^{-1}(z))$. Note that the function $\phi$, and so is its inverse $\phi^{-1}$, and so is the maximum eigenvalue function. Since the composition a convex function with a linear function is convex, $\tilde{\lambda}_{max}$ is a convex function of $z$.
%\end{proof}


%\begin{section}{Main Results}
%
%\begin{lemma}If I lose $\epsilon$ measure on the product, I lose $m \epsilon$ points on $\sphere_1$.
%\end{lemma}
%
%\begin{proof}Let $X:=\sphere_1 \times M$ then $\sigma^{n-1}(bad) = \epsilon \sigma^{n-1}(\sphere_1 \times M) = \gamma \\
%\sigma^{n-1}(bad) = \sum_{x \in X} \sigma^{n-1}(bad \sphere_1) \leq \epsilon \sigma^{n-1}(X)$
%$\gamma = \sum_{x \in X} \sigma^{n-1}_1(x)\sigma^{n-1}_2(M_x)$
%We have $\sigma^{n-1}_2(M_x) \geq \sigma^{n-1}_2(m_i) = k > 0$. \\
%$\gamma \geq \sum k \sigma^{n-1}_1(x)$.
%This means that $\sigma^{n-1}_1(B_x) \leq m \epsilon$.
%\end{proof}

%\begin{lemma}\label{lemma:Convex}The function
%\begin{equation}\label{funcDelta0}\Delta_\epsilon(X):=\sup \{d: \ball_d \subset \conv(\sphere_1 \setminus X), \sigma^{n-1}(X) = \epsilon, X \subset \sphere_1\},
%\end{equation}
%attains its minimum when $X$ is a convex set.
%\end{lemma}
%\substack{\text{Some long text that} \\ \text{should be multiline}}
We now prove our main result.
\begin{theorem}Let $X_{\epsilon'} = \{X \subset \sphere: \sigma^{n-1}(X) = \epsilon'\}$. Then, for any $\epsilon' \in (0,1)$, the function $\Delta(X)$ attains its minimum over $X_{\epsilon'}$ for some $X$ which is a spherical cap.
\end{theorem}

\begin{proof}We prove this via contradiction. Assume that there exists no spherical cap in $X_{\epsilon'}$ such that $\Delta(X)$ attains its minimum. This means there exists an $X^* \in X_{\epsilon'}$, where $X^*$ is not a spherical cap and $\argmin_{X \in X_{\epsilon'}}(\Delta(X))=X^*$. By Lemma \ref{lemma:constructSC} we can construct a spherical cap $\calC_{c,k}$ such that $\calC_{c,k} \subseteq X^*$ and $\calC_{c,k} = \Delta(X^*)$. Note that, we further have $\calC_{c,k} \subset X^*$, since $X^*$ is assumed not to be a spherical cap. This means that, there exists a spherical cap $\sigma^{n-1}(\calC_{c,k})$ such that $\sigma^{n-1}(\calC_{c,k}) < \epsilon'$. 

Then, the spherical cap $\calC_{c, \tilde{k}}$ with $\sigma^{n-1}(\calC_{c, \tilde{k}}) = \epsilon'$, satisfies $\tilde{k} < k$, due to Lemma \ref{lemma:muMonotone}. This implies $\Delta(\calC_{c, \tilde{k}}) < \Delta(\calC_{c, k}) = \Delta(X^*)$ due to Lemma \ref{lemma:deltaMonotone}. Therefore, $\Delta(\calC_{c, \tilde{k}}) < \Delta(X^*)$. This is a contradiction since we initially assumed that $\Delta(X)$ attains its minimum over $X_{\epsilon'}$ at $X^*$.
\end{proof}

\begin{theorem}Given a spherical cap $\calC_{c,k} \subseteq \sphere$ such that $\sigma^{n-1}(\calC_{c,k}) = \epsilon'$, 
\begin{equation*}\Delta(\calC_{c,k}) = \sqrt{(1-\alpha)}, \end{equation*}
where $\alpha := I^{-1}\left(\frac{\epsilon'\Gamma(\frac{d}{2})}{\pi^{d/2}}, \frac{d-1}{2}, \frac{1}{2}\right)$ and $\Gamma(x)=\int_{0}^{\infty} t^{x-1} e^{-t} \text{d}t$. Here $I^{-1}$ is the inverse incomplete beta function, i.e.,  $I^{-1}(y, a,b)= x$ where $I_x(a,b)=y$.
\end{theorem}

\begin{proof}Let $h:=1-\Delta(\calC_{c,k})$. It is well known \cite{sphericalCapRef} that the area of the spherical cap $\calC_{c,k} \subseteq \sphere$ is given by the equation:
\begin{equation}\epsilon' = \sigma^{n-1}(\calC_{c,k}) = \frac{\pi^{d/2}}{\Gamma[\frac{d}{2}]}I_{2h-h^2} \left(\frac{d-1}{2}, \frac{1}{2}\right),
\end{equation}
where $I$ is the incomplete beta function. 
From this, we get the following set of equations:
\begin{eqnarray}\nonumber \frac{\epsilon' \Gamma[\frac{d}{2}]}{\pi^{d/2}} &=& I_{2h-h^2}\left(\frac{d-1}{2}, \frac{1}{2}\right) \\
\nonumber 2h-h^2 &=&  I^{-1}\left(\frac{\epsilon'\Gamma(\frac{d}{2})}{\pi^{d/2}}, \frac{d-1}{2}, \frac{1}{2}\right) \\
\nonumber 2h-h^2 & = & \alpha \\
\label{lasteqn}h^2 -2h +\alpha &=& 0.
\end{eqnarray}
From \eqref{lasteqn}, we get $h=1\pm \sqrt{(1-\alpha)}$. Since $h\leq1$, we conclude that \mbox{$\Delta(\calC_{c,k}) = \sqrt{(1-\alpha)}$.} Note that, $\Delta(\calC_{c,k})$ only depends on $\epsilon$ for fixed $n$.
\end{proof}

\begin{corollary}For a given $\beta =  1 - \sum_{j=0}^d {{N}\choose{j}} \epsilon^j (1-\epsilon)^{N-j}$, we have \begin{equation*}\lim_{N \to \infty} \delta (\beta, N) = 1.\end{equation*}
\end{corollary}

\begin{proof} We first show that $\lim_{N \to \infty} \epsilon'_\beta(N) = 0.$
\begin{eqnarray} \nonumber 1- \beta = \sum_{j=0}^d {{N}\choose{j}} \epsilon^j (1-\epsilon)^{N-j} &\leq& (d+1) N^d (1 -\epsilon)^{N} \\
\nonumber \log{\left(\frac{1-\beta}{d+1}\right)} &\leq&  d \log N + N \log(1-\epsilon(N)) \\
\nonumber \frac{\log{\left(\frac{1-\beta}{d+1}\right)}}{d \log N} &\leq& 1 + \frac{N \log(1-\epsilon(N))}{d \log N} \\
\nonumber \lim_{N \to +\infty} \frac{\log{\left(\frac{1-\beta}{d+1}\right)}}{d \log N} &<& \lim_{N \to +\infty}  \frac{N \log(1-\epsilon(N))}{d \log N} \\
\label{limEpsilon} 0 &<& \lim_{N \to +\infty}  \frac{N \log(1-\epsilon(N))}{d \log N} 
\end{eqnarray}
We now prove by contradiction that \eqref{limEpsilon} implies that $\epsilon(N) \to 0$. Assume that $\lim_{N \to +\infty} \epsilon(N) \not= 0$. Let $g_1(N) := \frac{N}{d \log N}$ and $g_2(N):=\log (1-\epsilon(N))$. Note that $\lim_{N \to +\infty}g_1(N) = \infty$ and since $\epsilon(N) \in (0,1)$, we have $g_2(N) < 0$ for all $N$. Since  $\lim_{N \to +\infty} \epsilon(N) \not= 0,$ for there exists a subsequence $n_k$ of $\N$ such that $\lim_{k \to \infty} g_2(n_k) = -\epsilon$ for some $\epsilon > 0$. This means that there exists a subsequence $n_k$ of $\N$ such that $\lim_{k \to \infty}g_1(n_k)g_2(n_k) =  -\infty$, which implies that either the limit $\lim_{N \to +\infty}  \frac{N \log(1-\epsilon(N))}{d \log N}$ does not exist or it is $- \infty$, which is a contradiction since we have \eqref{limEpsilon}.

By continuity and monotonicity of $I^{-1}$ in its first parameter, $\delta = \sqrt{1-\alpha}$ tends to $1$ as $\epsilon \to 0$.

\end{proof}
%\end{section}
%spherical cap reference S. Li , 2011. Concise Formulas for the Area and Volume of a Hyperspherical Cap. Asian Journal of Mathematics & Statistics, 4: 66-70.

%
%\begin{section}{Campi Results} 
%We are interested in solving the following optimization problem:
%\begin{equation}
%\label{eqn:campiOpt0}
%\begin{aligned}
%& \underset{P}{\text{minimize}} & & \frac{\lambda_{\max}(P)}{\lambda_{\min}(P)} \\
%& \text{subject to} 
%&  & (A_is)^TP(A_is) \leq \gamma x^TPx, \quad \forall i = \{1,2, \ldots, m\},  \forall\, s \in \sphere,\\
%& && \lambda_{\max}(P) > 0 \\
%\end{aligned}
%\end{equation}
%Note that if $P$ is a solution to \eqref{eqn:campiOpt0}, then so is $k P$ for any $k \in \R_{\geq 0}.$ We can also say if the optimal solution of \eqref{eqn:campiOpt0} is $P^*$, where $ \frac{\lambda_{\max}(P)}{\lambda_{\min}(P)} = c^*$, then there exists a $\tilde{P}^*$ such that 
%$\lambda_{\min} \geq 1$. Therefore, we can rewrite \eqref{eqn:campiOpt0} as the following optimization problem:
%
%\begin{equation}
%\label{eqn:campiOpt1}
%\begin{aligned}
%& \underset{t, P}{\text{minimize}} & & t \\
%& \text{subject to} 
%&  & (A_is)^TP(A_is) \leq \gamma^2 x^TPx, \quad \forall i = \{1,2, \ldots, m\},  \forall\, s \in \sphere,\\
%& & &\lambda_{\max}(P) \leq t \\
%& & &  \lambda_{\min}(P) \geq 1.
%\end{aligned}
%\end{equation}
%
%We define $X = \sphere \times M$, and the linear isomorphism $\Phi: \mathbb{R}^{\frac{n(n-1)}{2}} \to \mathbb{S}^n$, which maps a vector of size $\frac{n(n-1)}{2}$ to an $n \times n$ symmetric matrix. Then, we can rewrite \eqref{eqn:campiOpt1} as:
%
%\begin{equation}
%\label{eqn:campiOpt1}
%\begin{aligned}
%& \underset{t, p}{\text{minimize}} & & t \\
%& \text{subject to} 
%& & f_\gamma(p, x) \leq 0, \forall x \in X,
%\end{aligned}
%\end{equation}
%where $f_\gamma(p, \xi)$ is defined as in:
%\begin{equation}\label{eqn:objFunction}f_\gamma(p, x) = \max\left((A_is)^T\Phi(p)(A_is) - \gamma x^T\Phi(p)x,\lambda_{\max}(\Phi(p)) - t, \lambda_{\max}(\Phi(-p)) +1\right).\end{equation}
%
%\begin{proposition}The function \eqref{eqn:objFunction} is convex in $p$.
%\end{proposition}
%
%\begin{proof}The function $\lambda_{\max}$ is a convex function of $P$, while $\Phi$ is linear in $p$. Therefore, $p \mapsto \Phi(\lambda_{\max}(p))$ is convex in $p$. Moreover, maximum of convex functions is also convex.
%\end{proof} 
%
%Note that the optimization problem \eqref{eqn:campiOpt1} has infinitely many constraints. We next consider the following optimization problem, $\Opt(\omega)$ where we sample $N$ constraints of \eqref{eqn:campiOpt1} independently and identically.
%
%\begin{equation}
%\label{eqn:campiOpt2}
%\begin{aligned}
%& \underset{t, p}{\text{minimize}} & & t \\
%& \text{subject to} 
%& & f_\gamma(p, x) \leq 0, \forall x \in \omega = \{x_1, x_2, \ldots, x_N\} \subset X.
%\end{aligned}
%\end{equation}
%
%\begin{definition}[Constraint violation probability] The constraint violation probability is defined as:
%\begin{equation*}
%V^*(\omega)=
%    \begin{cases}
%      \mathbb{P}\{x \in X: f(p^*(\omega), x) > 0\} &\text{if}\,\, \omega \in X^{N*},\\
%      1, & \text{otherwise}
%    \end{cases}
%\end{equation*}
%where $X^{N*}:=\{\omega \in X^N: \text{the solution of $\Opt(\omega)$ exists}\}$. 
%\end{definition}
%
%
%%Consider the semi-infinite optimization problem of the form:
%%\begin{equation*}
%%\begin{aligned}
%%& \underset{p}{\text{minimize}}
%%& & c^Tp \\
%%& \text{subject to}
%%& & f(p, x) \leq 0,\, \forall\, x \in X,
%%\end{aligned}
%%\end{equation*}
%%and the following optimization problem $\Opt(\omega)$ 
%%\begin{equation}
%%\label{eqn:sampledOptimization}
%%\begin{aligned}
%%& \underset{p}{\text{minimize}}
%%& & c^Tp \\
%%& \text{subject to}
%%& & f(p, x) \leq 0,\, \forall\, x \in \omega=\{x_1, \ldots x_N\} \subset X,
%%\end{aligned}
%%\end{equation}
%%where $X_N$ consists of $N$ independently and identically drawn samples from the set $X$. 
%
%
%\begin{theorem}[Campi]Consider the optimization problem $\Opt(\omega)$ given in \eqref{eqn:campiOpt1}. Let Assumption 1 and Assumption 2 hold \textcolor{red}{to be written}. Then, if $N \geq \frac{2}{\epsilon} (\ln \beta^{-1} + \frac{n(n-1)}{2} + 1)$, then $\mathbb{P}^N\{\{V^*(\omega) > \epsilon\} \cap X^{N*}\} \leq \beta$.
%\end{theorem}
%
%
%\begin{theorem}[Main Theorem]  Given $\beta$ and $N$, we can compute a $\delta$ such that $\rho < \frac{\delta}{\gamma}$ with probability $\beta \in [0,1]$.
%\end{theorem}



%\end{section}