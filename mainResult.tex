% !TEX root = main.tex
\textcolor{red}{Here, we need to talk about proving the Lyapunov decrement in finitely, many points, talk about campi and then make a transition}
In this section we suppose that we do not observe the modes, but only the pairs $(x_k,y_k).$
The idea is that if we have an ellipsoid that is contracted for all our observed pairs $(x,y),$  it is not enough to imply stability, because we observed only some possible behaviours of the dynamics.  However, suppose that we observed that the $\lambda-$contraction of the polytope $\cP$ is satisfied for many values of $x,$ then, it is tempting to extrapolate our observation, and claim that (noting $y=f(x) $ as always),

Let us consider $X = \sphere \times M$ the Cartesian product of the unit sphere $\sphere$ with $M$. Every element of $X$ can be written as $x = (s_x, k_x)$ with $s_x \in \sphere$ and $k_x \in M$. For notational simplicity, we drop the subscript $x$ whenever it is clear from the context. We define the classical projections of $X$ on the sphere and $M$ by $\pi_{\sphere}: X \to \sphere$ and $\pi_M: X \to M$.

We are interested in solving the following optimization problem for a given $\gamma \in (0,1)$:
\begin{equation}
\label{eqn:campiOpt0}
\begin{aligned}
& \text{find} & & P \\
& \text{subject to} 
&  & (A_is)^TP(A_is) \leq \gamma^2 s^TPs,\,\forall A_i \in \calM, \,\forall\, s \in \sphere,\\
& && P \succ 0. \\
\end{aligned}
\end{equation}
Note that, if such a $P$ exists, $\rho \leq \lambda$.
Note that if $P$ is a solution to \eqref{eqn:campiOpt0}, then so is $\alpha P$ for any $\alpha \in \R_{> 0}.$ Therefore, we can rewrite \eqref{eqn:campiOpt0} as the following optimization problem:

\begin{equation}
\label{eqn:campiOpt1}
\begin{aligned}
& \text{find} & & P \\
& \text{subject to} 
&  & (A_ks)^TP(A_ks) \leq \gamma^2 s^TPs,\, \forall A_i \in \calM, \,\forall\, s \in \sphere,\\
& & & P \succeq I.
\end{aligned}
\end{equation}

%We define the linear isomorphism $\Phi$ as the natural mapping \mbox{$\Phi: \mathbb{R}^{\frac{n(n+1)}{2}} \to \mathbb{S}^n$.} Using this mapping, for a fixed $\gamma \in (0, 1]$ we can rewrite \eqref{eqn:campiOpt01} as the following convex optimization problem:
%
%\begin{equation}
%\label{eqn:campiOpt1}
%\begin{aligned}
%& \text{find} & & p \\
%& \text{subject to} 
%& & f(p,x) \leq 0, \forall x \in X.
%\end{aligned}
%\end{equation}
%where $f(p,x) = \max(f_1(p, x), f_2(p))$, and 
%\begin{eqnarray*}
%f_1(p,x) &:=& (A_ks)^T\Phi(p)(A_ks) - \gamma^2 s^T\Phi(p)s \\
%f_2(p) &:=& \lambda_{\max}(\Phi(-p)) +1.
%\end{eqnarray*}

\textcolor{red}{We have to change the objective function!}
%$f(p, \xi)$ is defined as in:
%\begin{equation}\label{eqn:objFunction}f_\gamma(p, x) = \max\left((A_is)^T\Phi(p)(A_is) - \gamma x^T\Phi(p)x,\lambda_{\max}(\Phi(p)) - t, \lambda_{\max}(\Phi(-p)) +1\right).\end{equation}

%The optimization problem \eqref{eqn:campiOpt1} is convex in $p$.

%\begin{proof}The function $f_1(p,x)$ is clearly convex in $p$ for a fixed $x \in X$. The function 
%$\lambda_{max}: \mathbb{S}^n \to \R$ maps a symmetric positive matrix to its maximum eigenvalue. It is well-known that the function $\lambda_{\max}$ is a convex function of $P$. \cite{boyd}. This means that, $p \mapsto \Phi(\lambda_{\max}(p))$ is convex in $p$. Moreover, maximum of convex functions is also convex, which shows that $f(p, x)$ is convex in $p$.
%\end{proof}

Note that the optimization problem \eqref{eqn:campiOpt1} has infinitely many constraints. We next consider the following optimization problem where we sample $N$ constraints of \eqref{eqn:campiOpt1} independently and identically with the probability measure
$\mathbb{P}(A) = \frac{\mu(A)}{\mu(X)}, \forall\, A \in \Sigma$, where $N \geq d+1$, and $d := \frac{n(n+1)}{2}$. We denote this sampling by $\omega:=\{x_1, x_2, \ldots, x_N\} \subset X$, and obtain the following convex optimization problem $\Opt(\omega)$: 

\begin{equation}
\label{eqn:campiOpt2}
\begin{aligned}
& \text{find} & & p \\
& \text{subject to} 
& & f(p, x) \leq 0, \forall x \in \omega. \end{aligned}
\end{equation}

Let $p^*(\omega)$ be the solution of $\Opt(\omega)$. We are interested in the probability of $p^*(\omega)$ violating at least one constraint in the original problem \eqref{eqn:campiOpt1}. Therefore, we define constraint violation property next.

\begin{definition}The constraint violation probability is defined as:
\begin{equation*}
\calV^*(\omega)=
      \mathbb{P}\{x \in X: f(p^*(\omega), x) > 0\},\, \forall\, \omega \in X^N.
\end{equation*}
where $X^{N*}:=\{\omega \in X^N: \text{the solution of $\Opt(\omega)$ exists}\}$  (\cite{campi}). 
Note that, since we have $\mathbb{P}(A) = \frac{\mu(A)}{\mu(X)}$, we can rewrite this as:
\begin{equation*}
\calV^*(\omega)=
      \frac{\mu\{V\}}{\mu(X)},\, \forall\, \omega \in X^N,
\end{equation*}
where $V:=\{x \in X: f(p^*(\omega),x) > 0\}$, i.e., the set of points for which at least one constraint is violated.
\end{definition}

%We make the following assumptions on the problem $\Opt(\omega)$:
%\begin{enumerate}
%\item \label{assumption1} Uniqueness of solution: Note that this can be enforced by adding a tie-break rule of at most $\frac{n(n-1)}{2}$ convex conditions discriminating our solutions. 
%\item \label{assumption2} Nondegenaracy: with probability $1$, there is no redundancy in the constraint obtained from the sampling. 
%We can notice that a constraint obtained for a point $(s, k)$ gives us a constraint also on the point $(-s, k)$, and that a set of constraints containing some redundancy in them will be a set of constraints obtained from samples for a same mode. Now, let assume we consider for a fixed mode of index $k$ a nondegenerate sample of points $\{x_1, \dots, x_i\}$. Then if $x_{i+1} \notin \{(\pm s_{x_j},k), \ j \in \{1, \dots, i\} \}$ (which is a set of measure zero), the constraint obtained from $x_{i+1}$ is not redundant. Indeed, the mode of index $k$ might have all its eigenvalues but one with norm lower than $1$, and $x_{i+1}$ might be in the eigenspace of the latter eigenvalue (thus the constraint given by $x_{i+1}$ will be violated). Then, by considering the shortest distance between $x_{i+1}$ and the other points of the sample, we can choose for this eigenvalue a value close enough to $1$ (while being strictly greater than $1$) such that the constraints relative to the other points will still be satisfied.
%\end{enumerate}

The following theorem from \cite{campi} explicitly gives a relationship between $V^*(\omega)$ and $N$, $n$.
\begin{theorem}[from \cite{campi}]\label{thm:campi}Consider the optimization problem $\Opt(\omega)$ given in \eqref{eqn:campiOpt2}. \textcolor{red}{How to talk about assumptions here?} Then, for all $\epsilon \in (0,1)$ the following holds:
\begin{equation*}\mathbb{P}^N\{\{\calV^*(\omega) \leq \epsilon\} \} \geq 1- \sum_{j=0}^{d} \binom{N}{j}\epsilon^j (1-\epsilon)^{N-j}.\end{equation*}
\end{theorem}

Note that $\epsilon=1-I^{-1}(\beta, N-d, d+1)$ and can be interpreted as the ratio of the measure of points in $X$ that might violate at least one of the constraints in \eqref{eqn:campiOpt01} to the measure of all points in $X$.

We now state our main theorem which gives a probabilistic upper bound on JSR, and devote the next section to proving it step by step.

\begin{theorem}[Main Theorem] \label{thm:mainTheorem} For any $\eta > 0$, given $N \geq n+1$ and $\beta \in [0,1)$, we can compute $\delta(N) < \infty$ such that with probability at least $\beta$, $\rho \leq \frac{\gamma^*}{\delta}$. Moreover, $\lim_{N \to \infty} \delta(N) = 1$.
\end{theorem}