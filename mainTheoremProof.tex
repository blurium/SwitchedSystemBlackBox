% !TEX root = main.tex
In this section, we show how to compute an upper bound on $\rho$, with a user-defined confidence $\beta \in (0, 1)$. We do this by constructing a CQLF which is valid with probability at least $\beta$. Note that, the existence of a CQLF on a implies $\rho < 1$ due to Theorem \ref{jungersRho} and due to Property \ref{property:homogeneity}, it is enough to show that the CQLF is decreasing on a set enclosing the origin, e.g. $\sphere$. Therefore, to obtain an upper bound on $\rho$, we consider the following optimization problem:
\begin{equation}\label{eqn:campiOpt0}
\begin{aligned}
& \text{min}_{\gamma, P} & & \gamma \\
& \text{s.t.} 
&  & (Ax)^TP(Ax) \leq \gamma^2 x^TPx,\,\forall A \in \calM, \,\forall\, x \in \sphere,\\
& && P \succ 0. \\
\end{aligned}
\end{equation}
Note that, for all $A \in \calM$ the optimal $P$ and $\gamma^*$ satisfies $\frac{A}{\gamma^*}^TP\frac{A}{\gamma^*} \preceq P$. Therefore, $\rho\left(\frac{\calM}{\gamma^*}\right) \leq 1$, which leads to the upper bound $\rho\left(\calM \right) \leq \gamma^*$. However, solving the optimization problem \eqref{eqn:campiOpt0} is hard since it involves infinitely many constraints. Therefore, we instead sample $N$ initial states and modes uniformly random from the set $\sphere \times M$, and solve the following optimization problem with finitely many constraints instead:
\begin{equation}\label{eqn:campiOpt00}
\begin{aligned}
& \text{min}_{\gamma, P} & & \gamma \\
& \text{s.t.} 
&  & (Ax)^TP(Ax) \leq \gamma^2 x^TPx,\,\forall (x, j) \in \omega_N,\\
& && P \succ 0. \\
\end{aligned}
\end{equation}
where $\omega_N$ is a $N$-uniform random sampling of the set \mbox{$Z:=\sphere \times M$}.
Note that, since \eqref{eqn:campiOpt00} is convex for a fixed $\gamma$, we can perform bisection on $\gamma$ and solve a series of feasibility problems in $P$ instead. We now analyze the relationship between the solutions of the optimization problem \eqref{eqn:campiOpt0} and the following optimization problem:
\begin{equation}\label{eqn:campiOpt01}
\begin{aligned}
& \text{min}_{P \in \calP} & & \lambda_{\max}(P) \\
& \text{s.t.} 
&  & (A_j x)^TP(A_j x) \leq {\eta(N) \gamma^*}^2 x^TPx,\,\forall (x, j) \in \omega_N, \\
& && P \succeq I. \\
\end{aligned}
\end{equation}
where $\omega_N$ is a $N$-uniform random sampling of the set \mbox{$Z:=\sphere \times M$}, $\calP$ is a closed convex set, $\eta(N):=\left(1+\frac{\eta}{N}\right)$ for some $\eta > 0$, and $\gamma^*(\omega_N)$ is the optimal solution to the optimization problem \eqref{eqn:campiOpt00}. For the rest of the discussion, we refer to the optimization problem \eqref{eqn:campiOpt01} by $ \Opt(\omega_N)$. We denote its optimal solution by $P(\omega_N)$, where we drop the explicit dependence on $\omega_N$, when it is clear from the context. There are a few points that are worth noting about \eqref{eqn:campiOpt01}. Firstly, due to Property \ref{property:homogeneity}, we are able to replace the constraint $P \succ 0$ with the constraint $P \succeq I$. Moreover, for reasons that will become clear later in the discussion, we chose the objective function as $\lambda_{\max}(P)$, instead of solving a feasibility problem in $P$. Lastly, the additional factor is needed to ensure strict feasibility of \eqref{eqn:campiOpt01}, which will be helpful in the preceding discussion.s

The curious question whether the optimal solution of the sampled problem $\Opt(\omega_N)$ is a feasible solution to \eqref{eqn:campiOpt0} has been widely studied in the random convex optimization literature \cite{campi}. It turns out that under certain technical assumptions, the optimal solution of \eqref{eqn:campiOpt01} is feasible for the original problem \eqref{eqn:campiOpt0}, with some probability which is a function of the sample size $N$. To formalize this discussion, we define the constraint violation probability next.

\begin{definition}(from \cite{campi}) For all $\omega_N$ for which a solution to $\Opt(\omega_N)$ exists, the \emph{constraint violation probability} is defined as:
\begin{equation}\label{eqn:violation}
\calV^*(\omega_N)=
      \mathbb{P}\{z \in Z: f(P(\omega_N), z) > 0\}.
\end{equation}
Note that, since we have $\mathbb{P}(\calA) = \frac{\mu(\calA)}{\mu(Z)}$, we can rewrite \eqref{eqn:violation} as:
\begin{equation*}
\calV^*(\omega_N)=
      \frac{\mu(V)}{\mu(Z)},
\end{equation*}
where $\mu$ is a measure on $Z$ and $$V(\omega_N):=\{z \in Z: f(P(\omega_N), z) > 0\},$$ i.e., the set of points for which at least one constraint is violated for the sampling $\omega_N$.
\end{definition}

The following theorem from \cite{campi} gives an explicit relationship between $\calV^*(\omega_N)$, $N$, and $n$.
\begin{theorem}\label{thm:campi}Let $d:=\frac{n(n+1)}{2}+1$ and $N \geq d+1$. Consider the optimization problem $\Opt(\omega_N)$ given in \eqref{eqn:campiOpt01}. If $\Opt(\omega_N)$ satisfies the following technical assumptions:
\begin{enumerate}
\item When the problem $\Opt(\omega_N)$ admits an optimal solution, this solution is unique.
\item Problem $\Opt(\omega_N)$ is nondegenerate\footnote{\textcolor{red}{Explain this in a footnote maybe?}} with probability one.
\end{enumerate}
Then, for all $\epsilon \in (0,1)$ the following holds:
\begin{equation*}\mathbb{P}^N\{\calV^*(\omega_N) \leq \epsilon \} \geq 1- \sum_{j=0}^{d} \binom{N}{j}\epsilon^j (1-\epsilon)^{N-j}.\end{equation*}
\end{theorem}
Note that, $\epsilon$ is a constant can be interpreted as the ratio of the measure of points in $Z$ that might violate at least one of the constraints in \eqref{eqn:campiOpt01} to the measure of all points in $Z$.

\begin{proof}The proof is an immediate application of Theorem ?? in \cite{campi}, since the $\Opt(\omega_N)$
can be written as:
\begin{equation}
\label{eqn:campiOpt2}
\begin{aligned}
& \text{min}_{P \in \calP,\, t} & & t \\
& \text{s.t.} 
& & f_{\gamma^*}(P, z) \leq 0,\,\forall\ z \in Z\end{aligned}
\end{equation}
where $g_{\gamma^*}(P,z) = \max(g_1(P, z), g_2(P), g_3(P))$, and 
\begin{eqnarray*}
g_1(P, z) &:=& (A_j z)^TP(A_j z) - {\gamma^*}^2 z^TPz \\
g_2(P) &:=& \lambda_{\max}(-P) +1. \\
g_3(P) &:=&  \lambda_{\max}(P) - t.
\end{eqnarray*}
\end{proof}
We first note that, both of the assumptions in the statement of the theorem are technical and even if they do not hold can circumvented by slightly modifying the optimization problem \ref{eqn:campiOpt2}. We refer the interested reader to \cite{campi} for a more detailed discussion of such techniques. The objective function of \eqref{eqn:campiOpt2} is linear while each constraint is convex in $P$ for all $z \in Z$. Moreover, the set $\calP$ being closed and convex, without limiting the feasible region further we can assume $t$ lies in a closed interval as well. Also note that, the set of decision variables are in $\R^{\frac{n(n+1)}{2}+1}.$  Then, we can invoke Theorem ?? in \cite{campi} with the optimization problem \eqref{eqn:campiOpt2} to conclude the statement of the theorem.

Theorem \ref{thm:campi} states that the optimal solution of the sampled problem $\Opt(\omega_N)$ violates an $\epsilon$ fraction of the constraints in the original optimization problem  \eqref{eqn:campiOpt0} with probability $\beta$, where $\beta$ goes to $1$ as $N$ goes to infinity.

We next show that the measure of constraints that are violated in \eqref{eqn:campiOpt0} 

\begin{theorem}\label{thm:mainTheorem1}Consider a set of matrices $A \in \calM$, and an ellipsoid $E_P$, where with probability at least $\beta \in (0,1]$, $P$ satisfies:
\begin{equation}(A_j x)^TP(A_j x) \leq \gamma^*x^TPx,\,\, \forall\, x \in E_P \setminus Z_P, \forall\, j \in M\, 
\end{equation}
where $ Z_P \subset E_P$ and $\sigma^{n-1}(Z_P) = \epsilon$, we can compute $\delta$ such that:
\begin{equation}A_j E_P \subset \delta E_P.
\end{equation}
\end{theorem}

\begin{theorem}\label{thm:mainTheorem}Consider an $n$-dimensional switching system as in \eqref{eq:switchedSystem}. For any given $\beta \in (0,1)$ and a uniform random sampling $\omega_N \subset Z$, with $N \geq \frac{n(n+1)}{2}+1$, and let $\gamma^*(\omega_N) $ be the optimal solution to \eqref{eqn:campiOpt01}. Then, we can compute $\delta$ such that with probability at least $\beta$, we have:
$$\rho \leq \frac{\gamma^*(\omega_N)}{\delta}.$$
\end{theorem}


%Note that if $P$ is a solution of \eqref{eqn:campiOpt0}, due to Property \ref{property:homogeneity}, then so is $\alpha P$ for any $\alpha \in \R_{> 0}.$ Therefore, the constraint $P \succ 0$ can be replaced with the constraint $P \succeq I$. 
%In this section, we show that using Property \ref{property:homogeneity} and Property \ref{property:convpres}, by sampling finitely many points on a level set of a candidate CQLF, we can compute an upper bound on $\rho$. This section formalizes this discussion. Before proceeding to the main theorem, we motivate the upcoming technical discussion by stating the following theorem to which most of this section is devoted.
%
%\begin{theorem} \label{thm:mainTheorem0} Let $\epsilon \in (0,1]$, $\beta \in [0,1)$. Consider a uniform random sampling of $\sphere \times M$, denoted by $\omega_N$. Let $\gamma^*$ and $P$ be the optimal solution to:
%\begin{equation}\label{eqn:campiOpt01}
%\begin{aligned}
%& \text{min} & & \gamma \\
%& \text{s.t.} 
%&  & (A_j x)^TP(A_j x) \leq \gamma x^TPx,\,\forall (x, j) \in \omega_N \\
%& && P \succ 0. \\
%\end{aligned}
%\end{equation}
%Then for all $Z_P$ with $\sigma_P(Z_P)\leq \epsilon$, we have with probability at least $\beta$:
%\begin{equation} \label{eqn:exceptEps}(A_j x)^TP(A_j x)\leq \gamma^*x^TPx,\,\forall\, x \in E_P \setminus Z_P, \forall j \in M.\end{equation}
%Moreover, we can compute $\delta(\beta, \omega_N) < \infty$  such that:
%\begin{equation}\label{eqn:contraction}E_{\delta^2P} \subset  \convhull (E_{P} \setminus Z_{P}),
%\end{equation}
%and $\lim_{N \to \infty} \delta(\beta, \omega_N) = 1.$
%\end{theorem}
%
%Once Theorem \ref{thm:mainTheorem0} is established, the main result of this section follows.
%
%\begin{theorem}[Main Theorem] \label{thm:mainTheorem} Let $\omega_N$ be a uniform sampling of $\sphere \times M$, where $N \geq \frac{n(n+1)}{2}+1$ and $\beta \in [0,1)$. We can compute $\delta(\beta, \omega_N) < \infty$ such that with probability at least $\beta$ we have $$\rho \leq \frac{\gamma^*(\omega_N)}{\delta}.$$ Moreover, $\lim_{N \to \infty} \delta(\beta, \omega_N) = 1$.
%\end{theorem}
%
%%\begin{theorem}
%%Consider a black-box switching system and $N$ samples of its dynamics as in \eqref{eq:switchedSystem}. Consider the optimal solution $(\lambda^*,P)$ which minimizes $\lambda$ in \eqref{eq:lowerbound}. For any factor $1<\delta,$ one can compute the level of confidence $\beta$ such that $\rho<\delta\cdot\lambda^*.$ 
%%% denote $\gamma(P,\epsilon)$ the largest $\gamma$ such that $\gamma^2y_i^TPy_i\leq x_i^TPx_i$ 
%%\end{theorem}
%\begin{proof}Note that, by definition of $\gamma^*$ we have:
%\begin{equation*} (A_j x)^TP(A_j x) \leq \gamma^* x^TPx, \quad \forall\, (x, j)  \in \omega_N \end{equation*}
%for some $P \succ 0$. Then, by Theorem \ref{thm:mainTheorem0} we have:
%\begin{equation*} (A_j x)^TP(A_j x) \leq \gamma^*x^TPx, \quad \forall\, x \in E_P \setminus Z_P,\, \forall j \in M, \end{equation*}
%which can be rewritten as:
%\begin{equation*}\frac{\calM}{\gamma^*}(E_P \setminus Z_P) \subseteq E_P \setminus Z_P.
%\end{equation*}
%By Property \ref{property:convpres}, this implies:
%$$\frac{\calM}{\gamma^*} \conv(E_P \setminus Z_P) \subset \convhull(E_P \setminus Z_P).$$
%Then, due to Theorem \ref{thm:mainTheorem0} we also have \eqref{eqn:contraction}, meaning:
%$$\left(\frac{\cM}{\delta \gamma^*}\right) \convhull (E_P \setminus Z_P) \subset \convhull (E_P\setminus Z_P).$$
%Then, $\delta\gamma^*$ is an upper bound on $\rho,$ with probability at least~$\beta$. 
%\end{proof}
