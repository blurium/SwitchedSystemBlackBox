% !TEX root = main.tex
In this section, we show how to compute an upper bound on $\rho$, with a user-defined confidence $\beta \in [0, 1)$. We do this by constructing a CQLF which is valid with probability at least $\beta$. Note that, the existence of a CQLF on a implies $\rho < 1$ due to Theorem \ref{jungersRho} and due to Property \ref{property:homogeneity}, it is enough to show that the CQLF is decreasing on a set enclosing the origin, e.g. $\sphere$. Therefore, to obtain an upper bound on $\rho$, we consider the following optimization problem:
\begin{equation}\label{eqn:campiOpt0}
\begin{aligned}
& \text{min}_{\gamma, P} & & \gamma \\
& \text{s.t.} 
&  & (Ax)^TP(Ax) \leq \gamma^2 x^TPx,\,\forall A \in \calM, \,\forall\, x \in \sphere,\\
& && P \succ 0. \\
\end{aligned}
\end{equation}
Note that, for all $A \in \calM$ the optimal $P$ and the optimal $\gamma$ for the problem \eqref{eqn:campiOpt0} satisfies: $\frac{A}{\gamma}^TP\frac{A}{\gamma} \preceq P$. Therefore, $\rho\left(\frac{\calM}{\gamma}\right) \leq 1$, which leads to the upper bound $\rho\left(\calM \right) \leq \gamma$. However, solving the optimization problem \eqref{eqn:campiOpt0} is hard since it involves infinitely many constraints. Therefore, we instead sample $N$ initial states and modes uniformly random from the set $Z :=\sphere \times M$, and solve the following optimization problem with finitely many constraints instead:
\begin{equation}\label{eqn:campiOpt00}
\begin{aligned}
& \text{min}_{\gamma, P} & & \gamma \\
& \text{s.t.} 
&  & (Ax)^TP(Ax) \leq \gamma^2 x^TPx,\,\forall (x, j) \in \omega_N,\\
& && P \succ 0. \\
\end{aligned}
\end{equation}
where $\omega_N$ is a $N$-uniform random sampling of the set \mbox{$Z:=\sphere \times M$}.
Note that, since \eqref{eqn:campiOpt00} is convex for a fixed $\gamma$, we can perform bisection on $\gamma$ and solve a series of feasibility problems in $P$ instead. Therefore, we now analyze the relationship between the solutions of the optimization problem \eqref{eqn:campiOpt0} and the following optimization problem:
\begin{equation}\label{eqn:campiOpt01}
\begin{aligned}
& \text{min}_{P} & & \lambda_{\max}(P) \\
& \text{s.t.} 
&  & (A_j x)^TP(A_j x) \leq {((1 +\eta)\gamma^*)}^2 x^TPx,\,\forall (x, j) \in \omega_N, \\
& && P \succeq I. \\
\end{aligned}
\end{equation}
where $\omega_N$ is a $N$-uniform random sampling of the set $Z$, $\eta > 0$, and $\gamma^*$ is the optimal solution to the optimization problem \eqref{eqn:campiOpt00}. For the rest of the discussion, we refer to the optimization problem \eqref{eqn:campiOpt01} by $ \Opt(\omega_N)$. We denote its optimal solution by $P(\omega_N)$ and $\gamma^*(\omega_N)$. We drop the explicit dependence of $P$ on $\omega_N$ when it is clear from the context. There are a few points that are worth noting about \eqref{eqn:campiOpt01}. Firstly, due to Property \ref{property:homogeneity}, we are able to replace the constraint $P \succ 0$ with the constraint $P \succeq I$. Moreover, for reasons that will become clear later in the discussion, we chose the objective function as $\lambda_{\max}(P)$, instead of solving a feasibility problem in $P$. Lastly, the additional $\eta$ factor is introduced to ensure strict feasibility of \eqref{eqn:campiOpt01}, which will be helpful in the preceding discussion.

The curious question whether the optimal solution of the sampled problem $\Opt(\omega_N)$ is a feasible solution to \eqref{eqn:campiOpt0} has been widely studied in the random convex optimization literature \cite{campi}. It turns out that under certain technical assumptions, the optimal solution of \eqref{eqn:campiOpt01} is feasible for the original problem \eqref{eqn:campiOpt0}, with some probability which is a function of the sample size $N$. To formalize this discussion, we define the constraint violation probability next.

\begin{definition}(from \cite{campi}) For all $\omega_N$ for which a solution to $\Opt(\omega_N)$ exists, the \emph{constraint violation probability} is defined as:
\begin{equation}\label{eqn:violation0}
\calV^*(\omega_N)=
      \mathbb{P}\{z \in Z: f(P(\omega_N), z) > 0\}.
\end{equation}
Note that, since we have $\mathbb{P}(\calA) = \frac{\mu(\calA)}{\mu(Z)}$, we can rewrite \eqref{eqn:violation0} as:
\begin{equation*}
\calV^*(\omega_N)=
      \frac{\mu(V(\omega_N))}{\mu(Z)},
\end{equation*}
where $V(\omega_N):=\{z \in Z: f(P(\omega_N), z) > 0\},$ i.e., the set of points for which at least one constraint is violated for the sampling $\omega_N$.
\end{definition}
%
%The following theorem from \cite{campi} gives an explicit relationship between $\calV^*(\omega_N)$, $N$, and $n$.
%\begin{theorem}\label{thm:campi}Let $d:=\frac{n(n+1)}{2}+1$ and $N \geq d+1$. Consider the optimization problem $\Opt(\omega_N)$ given in \eqref{eqn:campiOpt01}. If $\Opt(\omega_N)$ satisfies the following technical assumptions:
%\begin{enumerate}
%\item When the problem $\Opt(\omega_N)$ admits an optimal solution, this solution is unique.
%\item Problem $\Opt(\omega_N)$ is nondegenerate\footnote{\textcolor{red}{Explain this in a footnote maybe?}} with probability one.
%\end{enumerate}
%Then, for all $\epsilon \in (0,1)$ the following holds:
%\begin{equation*}\mathbb{P}^N\{\calV^*(\omega_N) \leq \epsilon \} \geq 1- \sum_{j=0}^{d} \binom{N}{j}\epsilon^j (1-\epsilon)^{N-j}.\end{equation*}
%\end{theorem}
%Note that, $\epsilon$ is a constant can be interpreted as the ratio of the measure of points in $Z$ that might violate at least one of the constraints in \eqref{eqn:campiOpt01} to the measure of all points in $Z$.

\begin{theorem}\label{mainTheorem0}
Let $d:=\frac{n(n+1)}{2}+1$ and $N \geq d+1$. Consider the optimization problem $\Opt(\omega_N)$ given in \eqref{eqn:campiOpt01}, where $\omega_N$ is a uniform random sampling of the set $Z$. If $\Opt(\omega_N)$ satisfies the following technical assumptions:
\begin{enumerate}
\item When the problem $\Opt(\omega_N)$ admits an optimal solution, this solution is unique.
\item Problem $\Opt(\omega_N)$ is nondegenerate\footnote{\textcolor{red}{Explain this in a footnote maybe?}} with probability one.
\end{enumerate}
Then, for all $\epsilon \in (0,1)$ the following holds:
\begin{equation}\label{eqn:violation}\mathbb{P}^N\left\{ \mu(V(\omega_N)) \leq \epsilon \right\} \geq 1- \sum_{j=0}^{d} \binom{N}{j}\epsilon^j (1-\epsilon)^{N-j}.\end{equation}
\end{theorem}

\begin{proof}The proof is an immediate application of Theorem ?? in \cite{campi}, since the $\Opt(\omega_N)$
can be written as:
\begin{equation}
\label{eqn:campiOpt2}
\begin{aligned}
& \text{min}_{P, t} & & t \\
& \text{s.t.} 
& & f_{\gamma^*}(P, z) \leq 0,\,\forall\ z \in Z\end{aligned}
\end{equation}
where $g_{\gamma^*}(P,z) = \max(g_1(P, z), g_2(P), g_3(P))$, and 
\begin{eqnarray*}
g_1(P, z) &:=& (A_j z)^TP(A_j z) - {\gamma^*}^2 z^TPz \\
g_2(P) &:=& \lambda_{\max}(-P) +1. \\
g_3(P) &:=&  \lambda_{\max}(P) - t.
\end{eqnarray*}
We first note that, both of the assumptions in the statement of the theorem are technical and even if they do not hold for the optimization problem \ref{eqn:campiOpt2}, it is possible to obtain . We refer the interested reader to \cite{campi} for a more detailed discussion of such techniques. The objective function of \eqref{eqn:campiOpt2} is linear while each constraint is convex in $P$ for all $z \in Z$. Also note that, the set of decision variables are in $\R^{\frac{n(n+1)}{2}+1}.$  Then, we can invoke Theorem ?? in \cite{campi} with the optimization problem \eqref{eqn:campiOpt2} to conclude the statement of the theorem.
\end{proof}

Theorem \ref{mainTheorem0} states that the optimal solution of the sampled problem $\Opt(\omega_N)$ violates an $\epsilon$ fraction of the constraints in the original optimization problem  \eqref{eqn:campiOpt0} with probability $\beta$, where $\beta$ goes to $1$ as $N$ goes to infinity.

Before proceeding to the next proof, we present the following lemma which will be helpful.
\begin{remark} (see e.g. \cite{aliprantis}) \label{rem:mappingMeasures}
Let $X \subset \R^n$ and $L \in \calL(\R^n)$. Then we have:
\begin{equation*}
\lambda(\calL(X)) = |\det(L)| \lambda(X).
\end{equation*}
\end{remark}

\begin{theorem}\label{thm:mainTheorem01}Let $\gamma \in \R_{> 0}$. Consider a set of matrices $A \in \calM$, and the matrix $P$, satisfying:
\begin{equation}\label{eqn:P}(A_j x)^TP(A_j x) \leq {\gamma}^2x^TPx,\,\, \forall\, (x, j) \in Z \setminus V,\end{equation}
for some $V \subset Z$ where $\mu(V) \leq \epsilon$. Then, the following also holds:
\begin{equation*}\label{eqn:thm01-2}(\bar{A_j} x)^T(\bar{A_j} x) \leq {\gamma}^2x^Tx,\,\, \forall\, x \in \sphere \setminus \sphere', \forall\, j \in M,\end{equation*}
for some $\sphere' \subset \sphere$ where $\bar{A_j} = L^{-1}A_jL$, where $L$ is defined as in \eqref{choleski} and $$\sigma(\sphere') \leq m\epsilon \kappa(P),$$
where $\kappa(P) = \sqrt{\frac{\lambda_{\max}(P)}{\det(P)^n}}$.
\end{theorem}

\begin{proof}
Note that $V \subset \Sigma$. Let $V_{\sphere} = \pi_{\sphere}(V)$ and $V_M = \pi_M (V)$. We notice that $\Sigma_M$ is the disjoint union of its $2^m$ elements $\{\calM_i, i \in \{1,2, \ldots 2^m\} \}$. Then $V$ can be written as the disjoint union $V = \sqcup_{1 \leq i \leq 2^m} (\calS_i, \calM_i)$ where $\calS_i \in \Sigma(\sphere)$. We notice that 
$V_{\sphere} = \sqcup_{1 \leq i \leq 2^m} \calS_i$, 
and
\begin{equation*}
\sigma^{n-1} (V_{\sphere}) = \sum_{1 \leq i \leq 2^m} \sigma^{n-1} (\calS_i).
\end{equation*}
We have 
\begin{eqnarray*}
\mu(V) &=& \mu(\sqcup_{1 \leq i \leq 2^m} (\calS_i, \calM_i)) \\
&=& \sum_{1 \leq i \leq 2^m} \mu(\calS_i, \calM_i) \\
 &=& \sum_{1 \leq i \leq 2^m} \sigma^{n-1} \otimes \mu_M (\calS_i, \calM_i) \\
 & = &\sum_{1 \leq i \leq 2^m} \sigma^{n-1}(\calS_i) \mu_M (\calM_i).
\end{eqnarray*}
Note that we have $$\min_{j \in M} \mu_M(\{j\}) = \frac{1}{m}.$$ Then since $ \forall \ i$, $\mu_M(\calM_i) \geq \frac{1}{m}$, we get:
\begin{equation}
\sigma^{n-1}(V_{\sphere}) \leq \frac{\mu(V)}{\frac{1}{m}} \leq m \epsilon.
\end{equation}
This means that 
\begin{equation}\label{eqn:P2}(A_j x)^TP(A_j x) \leq {\gamma}^2x^TPx,\,\, \forall\, x \in \sphere \setminus V_S, \,\forall\, m \in M,\end{equation}
where $\sigma^{n-1}(V_S) \leq m \epsilon.$

We then perform the change of coordinates defined by $L^{-1} \in \calL(\R^n)$ which maps $\sphere$ to $P$ defined as in \eqref{choleski}. We can then rewrite
\eqref{eqn:P2} in this new coordinates system as in: 
\begin{equation}\label{eqn:P}(\bar{A}_j x)^T(\bar{A}_j x) \leq {\gamma}^2x^Tx,\,\, \forall\, x \in E_P\setminus L^{-1}(V_S), \,\forall\, m \in M.\end{equation}

Due to the  the homogeneity Property \ref{property:homogeneity}, this implies
\begin{equation}\label{eqn:P}(\bar{A}_j x)^T(\bar{A}_j x) \leq {\gamma}^2x^Tx,\,\, \forall\, x \in \sphere\setminus \proj_S(L^{-1}(V_S)), \,\forall\, m \in M.\end{equation}

%We now show how to relate $\sigma^{n-1}({V_S})$ to $\sigma^{n-1}(\proj_S(L^{-1}(V_S)))$. Note that, since $L \in \calL({\R^n})$ by Remark \ref{rem:mappingMeasures} we have:
%\begin{equation}\label{eqn:det}\sigma_P(L^{-1}(V_S)) = |\det(L^{-1})| \sigma^{n-1}(V_S).\end{equation}

We now show how to relate $\sigma^{n-1}(V_S)$ to $\sigma^{n-1}(\proj_S(L^{-1}(V_S)))$. Consider $S^{V_{\sphere}}$ the sector of $\ball$ defined by $V_{\sphere}$. We denote $C:= L^{-1}(S^{V_{\sphere}})$ and $V':=\proj_S(L^{-1}(V_S))$. We have $\proj_{\sphere}(C) = V'$ and $S^{V'} \subset \calH_{1/ \lambda_{\min}(L^{-1})}(C)$.  This leads to:
$$\sigma^{n-1}(V') = \lambda(S^{V'}) \leq \lambda (\calH_{1/ \lambda_{\min}(L^{-1})}(C)),$$ which means the following holds:
\begin{eqnarray}\nonumber\sigma^{n-1}(V') &\leq& \frac{1}{\lambda_{\min}(L^{-1})^n} \lambda( L^{-1}(S^{V_{\sphere}}))\\ \nonumber &=&\frac{|\det(L^{-1})|}{\lambda_{\min}(L^{-1})^n}\sigma(S^{V_S}),\\
 \label{eqn:map1} &=&\sqrt{\frac{\lambda_{\max}(P)^n}{\det(P)}}\sigma^{n-1}(V_S)
\end{eqnarray}
where the first equality follows from Remark \ref{rem:mappingMeasures}.
Putting together \eqref{eqn:map1}, \eqref{eqn:P2}, \label{eqn:P}, we get the statement of the theorem where $S' = \proj_S(L^{-1}(V_S)).$
%Then, if we let $E':=L^{-1} V_S$, where $L$ is the linear transformation mapping $E_P$ to $\sphere$, we get: $$\sigma_P(E') = \sigma^{n-1}(V_S), and the result of the theorem follows.
\end{proof}


\begin{lemma}\label{lemma:eps}Let $\epsilon \in (0, 1)$. Then, we can compute $\alpha(\epsilon)$ satisfying:
\begin{equation}\label{eqn:ballinc}\alpha(\epsilon) = \sup\{r: r\ball \subset \conv(\sphere \setminus X_\epsilon),
\end{equation}
for all $X_\epsilon \in \{X \subset \sphere: \sigma^{n-1}(X) \leq \epsilon\}$. 
%Moreover, $\lim_{\epsilon \to 0} \alpha(\epsilon) = 1$.
\end{lemma}

\begin{proof}See Appendix.
\end{proof}

\begin{lemma}\label{lemma:epsilon1}Let $\epsilon \in (0, 1)$ and $\gamma \in \R_{> 0}$. Consider the set of matrices and $A \in \calM$ satisfying:
\begin{equation}\label{eqn:contraction}(A_jx)^T(A_jx) \leq \gamma x^Tx, \quad \forall\, x \in \sphere \setminus \sphere', \forall\,j \in, M,\end{equation}
where $\sphere' \subset \sphere$ and $\sigma^{n-1}(\sphere') \leq \epsilon$, then we have:
\begin{equation*}
\rho(\calM) \leq \frac{\gamma}{\alpha(\epsilon)}
\end{equation*}
where $\alpha(\epsilon)$ is defined as in \eqref{eqn:ballinc}.
\end{lemma}

\begin{proof}Note that, \eqref{eqn:contraction} implies that:
$$A_j (\sphere \setminus \sphere') \subset \gamma \ball.$$
Using Property \ref{property:convpres} this also implies:
$$A_j \convhull(\sphere \setminus \sphere') \subset \convhull(A_j(\sphere \setminus \sphere')) \subset \gamma^* \ball .$$
By Lemma \ref{lemma:eps} we have:
$$A_j (\alpha(\epsilon) \ball) \subset A_j (\convhull (\sphere \setminus \sphere')) \subset \ball, \quad  \forall\, j \in M,$$
where $\alpha(\epsilon)$ is defined as in \eqref{eqn:alphaEpsilon}. Therefore, we get:
$$\alpha(\epsilon) A_j (\ball) \subset \gamma\ball.$$
which implies that $\rho(\calM) \leq \frac{\gamma}{\alpha(\epsilon)}.$
\end{proof}


%\begin{theorem}\label{thm:mainTheorem1}Let $\epsilon \in (0,1)$ and $\gamma \in \R_{> 0}$. Consider a set of matrices $A \in \calM$, and an ellipsoid $E_P$ satisfying:
%\begin{equation}\label{eqn:ellipsoid}(A_j x)^TP(A_j x) \leq \gamma^2x^TPx,\,\, \forall\, x \in E_P \setminus E', \forall\, j \in M, \end{equation}
%for some $E' \subset E$ and $\sigma_P(E') \leq \epsilon$, then we can compute $\bar{\alpha}(\epsilon) > 0$ such that we have:
%\begin{equation}\label{eqn:contractionEllipsoid}A_j \tilde{E}_P \subset \frac{\gamma}{\bar{\alpha}(\epsilon)}\tilde{E}_P, \forall\, j \in M.
%\end{equation}
%\end{theorem}
%where, $\lim_{\epsilon \to 0} \bar{\alpha}(\epsilon) = 1$.


%\begin{proof} We first rewrite the inequality in \eqref{eqn:ellipsoid} under the linear transformation $L$ which maps the ellipsoid $E_P$ to $\sphere$ as follows:
%\begin{equation}(\bar{A}_j x)^T(\bar{A}_j x) \leq \gamma^2x^Tx,\,\, \forall\, x \in \sphere \setminus \sphere', \forall\, j \in M, \end{equation}
%
%
%Due to Property \ref{property:homogeneity}, the inequality in \eqref{eqn:ellipsoid} implies
%\begin{equation}(A_j x)^TP(A_j x) \leq \gamma^2x^TPx,\,\, \forall\, x \in \sphere \setminus \proj_S(E'), \forall\, j \in M, \end{equation}
%
%
%
%Let $V_S:=\proj_S(V)$. Let us consider $S^{V_{\sphere}}$ the sector of $\ball$ defined by $V_{\sphere}$. We denote $C:= L(S^{V_{\sphere}})$. We have $\proj_{\sphere}(C) = V'$ and $S^{V'} \subset \calH_{1/ \lambda_{\min}(L^{-1})}(C)$.  We have then 
%$$\sigma^{n-1}(V') = \lambda(S^{V'}) \leq \lambda (\calH_{1/ \lambda_{\min}(L^{-1})}(C)),$$ which means: $$\sigma^{n-1}(V') \leq \frac{1}{\lambda_{\min}(L^{-1})^n} \lambda(C).$$ Using Remark \ref{rem:mappingMeasures}, we have the result of the theorem.
%\end{proof}

%
%\begin{theorem}\label{thm:mainTheorem}Consider an $n$-dimensional switching system as in \eqref{eq:switchedSystem}. For any given $\beta \in (0,1]$ and a uniform random sampling $\omega_N \subset Z$, with $N \geq \frac{n(n+1)}{2}+1$, and let $\gamma^*(\omega_N) $ be the optimal solution to \eqref{eqn:campiOpt01}. Then, we can compute $\delta(\beta, \omega_N)$, such that with probability at least $\beta$ we have:
%$$\rho \leq \frac{\gamma^*(\omega_N)}{\delta(\beta, \omega_N)},$$
%where $\lim_{N \to \infty}\delta(\beta, \omega_N) = 1$.
%\end{theorem}
%
%\begin{proof}
%Note that, by definition of $\gamma^*(\omega_N)$ we have:
%\begin{equation*} (A_j x)^TP(A_j x) \leq {\gamma^*}^2 x^TPx, \quad \forall\, (x, j)  \in \omega_N \end{equation*}
%for some $P \succ 0$. 
%Note that the inequality \eqref{eqn:violation} in Theorem \ref{mainTheorem0} can be also written as:
%\begin{equation}\label{eqn:violation2}\mathbb{P}^N\left\{ \mu(V(\omega_N)) \leq \epsilon \right\} \geq I(\epsilon; d+1, N-d),\end{equation}
%where $I(\ell;a,b)$ is the regularized in complete beta function. Then, for all $\epsilon \in (0,1)$ satisfying:
%\begin{equation}\label{eqn:eps}\epsilon \leq 1- I^{-1}(\beta, d+1, N-d),\end{equation} we have $\mathbb{P}^N\left\{ \mu(V(\omega_N)) \leq \epsilon \right\} \geq \beta$.
%Then, by Theorem \ref{thm:mainTheorem01} for all $\epsilon$ satisfying \eqref{eqn:eps}, with probability at least $\beta$ the following holds:
%\begin{equation*} (A_jx)^TP(A_jx) \leq  {\gamma^*}^2 x^TPx, \quad \forall (x, j) \in Z \setminus V,\end{equation*}
%for some $V \subset Z$, where $\mu(Z) \leq \epsilon$. By Theorem \ref{thm:mainTheorem01}, this implies that with probability at least $\beta$ the following also holds:
%\begin{equation} \label{eqn:mainthm1}(A_jx)^TP(A_jx) \leq  {\gamma^*}^2 x^TPx, \quad \forall x \in E_P \setminus E', \forall\, j \in M,\end{equation}
%for some $E'$ where $\sigma_P(E') \leq m \epsilon$. The inequality in \eqref{eqn:mainthm1} can be rewritten as:
%\begin{equation}\label{eqn:mainthm2}\frac{A_j}{\gamma^*}(E_P \setminus E') \subset \tilde{E}_P,\quad \forall j \in M.
%\end{equation}
%Note that, $E_P \subset \tilde{E}_P$. Using this fact, along with \eqref{eqn:mainthm2} and Property \ref{property:convpres}, we get:
%\begin{eqnarray*}\frac{A_j}{\gamma^*}\convhull(E_P \setminus E') &\subset& \convhull \left(\frac{A_j}{\gamma^*} \left(E_P \setminus E'\right) \right)\\ &\subset& \convhull\left(\frac{A_j}{\gamma^*}\tilde{E}_P\right), \,\, \forall\, j \in M.
%\end{eqnarray*}
%
%
%
%$$\frac{\calM}{\gamma^*} \conv(E_P \setminus E') \subset \convhull(E_P \setminus E'),$$
%for some $E' \subset E_P$, where $\sigma_P(E') \leq \epsilon$.
%Then, due to Theorem \ref{thm:mainTheorem0} we also have \eqref{eqn:contraction}, meaning:
%$$\left(\frac{\cM}{\delta \gamma^*}\right) \convhull (E_P \setminus X_P) \subset \convhull (E_P\setminus X_P).$$
%Then, $\delta\gamma^*$ is an upper bound on $\rho,$ with probability at least~$\beta$. 
%\end{proof}


%Note that if $P$ is a solution of \eqref{eqn:campiOpt0}, due to Property \ref{property:homogeneity}, then so is $\alpha P$ for any $\alpha \in \R_{> 0}.$ Therefore, the constraint $P \succ 0$ can be replaced with the constraint $P \succeq I$. 
%In this section, we show that using Property \ref{property:homogeneity} and Property \ref{property:convpres}, by sampling finitely many points on a level set of a candidate CQLF, we can compute an upper bound on $\rho$. This section formalizes this discussion. Before proceeding to the main theorem, we motivate the upcoming technical discussion by stating the following theorem to which most of this section is devoted.
%
%\begin{theorem} \label{thm:mainTheorem0} Let $\epsilon \in (0,1]$, $\beta \in [0,1)$. Consider a uniform random sampling of $\sphere \times M$, denoted by $\omega_N$. Let $\gamma^*$ and $P$ be the optimal solution to:
%\begin{equation}\label{eqn:campiOpt01}
%\begin{aligned}
%& \text{min} & & \gamma \\
%& \text{s.t.} 
%&  & (A_j x)^TP(A_j x) \leq \gamma x^TPx,\,\forall (x, j) \in \omega_N \\
%& && P \succ 0. \\
%\end{aligned}
%\end{equation}
%Then for all $Z_P$ with $\sigma_P(Z_P)\leq \epsilon$, we have with probability at least $\beta$:
%\begin{equation} \label{eqn:exceptEps}(A_j x)^TP(A_j x)\leq \gamma^*x^TPx,\,\forall\, x \in E_P \setminus Z_P, \forall j \in M.\end{equation}
%Moreover, we can compute $\delta(\beta, \omega_N) < \infty$  such that:
%\begin{equation}\label{eqn:contraction}E_{\delta^2P} \subset  \convhull (E_{P} \setminus Z_{P}),
%\end{equation}
%and $\lim_{N \to \infty} \delta(\beta, \omega_N) = 1.$
%\end{theorem}
%
%Once Theorem \ref{thm:mainTheorem0} is established, the main result of this section follows.
%
%\begin{theorem}[Main Theorem] \label{thm:mainTheorem} Let $\omega_N$ be a uniform sampling of $\sphere \times M$, where $N \geq \frac{n(n+1)}{2}+1$ and $\beta \in [0,1)$. We can compute $\delta(\beta, \omega_N) < \infty$ such that with probability at least $\beta$ we have $$\rho \leq \frac{\gamma^*(\omega_N)}{\delta}.$$ Moreover, $\lim_{N \to \infty} \delta(\beta, \omega_N) = 1$.
%\end{theorem}
%
%%\begin{theorem}
%%Consider a black-box switching system and $N$ samples of its dynamics as in \eqref{eq:switchedSystem}. Consider the optimal solution $(\lambda^*,P)$ which minimizes $\lambda$ in \eqref{eq:lowerbound}. For any factor $1<\delta,$ one can compute the level of confidence $\beta$ such that $\rho<\delta\cdot\lambda^*.$ 
%%% denote $\gamma(P,\epsilon)$ the largest $\gamma$ such that $\gamma^2y_i^TPy_i\leq x_i^TPx_i$ 
%%\end{theorem}
%\begin{proof}Note that, by definition of $\gamma^*$ we have:
%\begin{equation*} (A_j x)^TP(A_j x) \leq \gamma^* x^TPx, \quad \forall\, (x, j)  \in \omega_N \end{equation*}
%for some $P \succ 0$. Then, by Theorem \ref{thm:mainTheorem0} we have:
%\begin{equation*} (A_j x)^TP(A_j x) \leq \gamma^*x^TPx, \quad \forall\, x \in E_P \setminus Z_P,\, \forall j \in M, \end{equation*}
%which can be rewritten as:
%\begin{equation*}\frac{\calM}{\gamma^*}(E_P \setminus Z_P) \subseteq E_P \setminus Z_P.
%\end{equation*}
%By Property \ref{property:convpres}, this implies:
%$$\frac{\calM}{\gamma^*} \conv(E_P \setminus Z_P) \subset \convhull(E_P \setminus Z_P).$$
%Then, due to Theorem \ref{thm:mainTheorem0} we also have \eqref{eqn:contraction}, meaning:
%$$\left(\frac{\cM}{\delta \gamma^*}\right) \convhull (E_P \setminus Z_P) \subset \convhull (E_P\setminus Z_P).$$
%Then, $\delta\gamma^*$ is an upper bound on $\rho,$ with probability at least~$\beta$. 
%\end{proof}
